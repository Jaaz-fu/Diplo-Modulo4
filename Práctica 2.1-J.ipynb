{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge langdetect    ---ya no use esta librería"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np    \n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Párrafo 1\n",
    "text_01 = \"Hola, hola, hola hola\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Párrafo 2\n",
    "text_02 = \"Hi, this class is very interesting, I like to program, but I'm not very good yet.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Párrafo 3\n",
    "text_03 = \"Γεια σου, αυτή η τάξη είναι πολύ ενδιαφέρουσα, μου αρέσει να προγραμματίζω, αλλά δεν είμαι πολύ καλός ακόμα.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://twitter.com/GaeboraKae/status/1169424976413372416\n",
    "text_04 = \"こんにちは、このクラスは非常に興味深いです、私はプログラミングが好きですが、私はまだあまりよくありません\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://twitter.com/VICEenEspanol/status/1169705966234943488\n",
    "text_05 = \"Привет, этот класс очень интересный, я люблю программировать, но я еще не очень хорош.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://twitter.com/Anaro74/status/1169579828963463168\n",
    "text_06 = \"Salve, hoc genus est valde interesting, ego similis programming, sed tamen Im 'non valde bona.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenización, Creación de la BoW, Matriz Termino Documento (binaria) y Matriz Término Documento (frecuencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tw = [text_01,text_02,text_03,text_04,text_05,text_06]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_bow_mtdb(x):\n",
    "    text_tokens = []\n",
    "    text_tokens_wout_stopwords = []\n",
    "    dicc_termns = {}\n",
    "    for i in x:\n",
    "        if TextBlob(i).detect_language() != \"en\":\n",
    "           a= str(TextBlob(i).translate(to = \"en\"))\n",
    "           text_tokens.append(tokenizer.tokenize(a.lower())) #tokenizar y quitar signos de puntuación\n",
    "        else : text_tokens.append(tokenizer.tokenize(i.lower()))\n",
    "    \n",
    "    for i in text_tokens:\n",
    "        wout_stop = []\n",
    "        for word in i:\n",
    "            if word not in stopwords.words('english'): \n",
    "                wout_stop.append(word)\n",
    "                if(word in dicc_termns):#incrementar palabras al diccionario\n",
    "                    dicc_termns[word] = dicc_termns[word] + 1\n",
    "                elif(word not in dicc_termns):#agregar palabras al diccionario        \n",
    "                     dicc_termns[word] = 1\n",
    "        text_tokens_wout_stopwords.append(wout_stop)\n",
    "        \n",
    "    matrix_1 = np.zeros((len(text_tokens_wout_stopwords), len(dicc_termns)))# Pre-allocate matrix\n",
    "    matrix_2 = np.zeros((len(text_tokens_wout_stopwords), len(dicc_termns)))# Pre-allocate matrix\n",
    "    i = 0\n",
    "    for word_termns in dicc_termns: #dicc_termns todos los términos\n",
    "        j = 0\n",
    "        for word_texts in text_tokens_wout_stopwords:#listado de todos los textos\n",
    "            matrix_2[j,i] = word_texts.count(word_termns) \n",
    "            if(word_termns in word_texts): #si está\n",
    "                matrix_1[j, i] = 1 \n",
    "            j = j + 1  \n",
    "        i = i + 1    \n",
    "    return(text_tokens_wout_stopwords,dicc_termns,matrix_1,matrix_2)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_Bow_MTDB = token_bow_mtdb(list_tw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Twits tokenizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "4\n",
      "7\n",
      "7\n",
      "[['hello', 'hello', 'hello', 'hello'], ['hi', 'class', 'interesting', 'like', 'program', 'good', 'yet'], ['hi', 'class', 'interesting', 'like', 'plan', 'good', 'yet'], ['hello', 'class', 'interesting', 'like', 'programming', 'still', 'good'], ['hi', 'class', 'interesting', 'like', 'programming', 'good', 'yet'], ['hello', 'race', 'interesting', 'like', 'programming', 'still', 'good']]\n"
     ]
    }
   ],
   "source": [
    "tokenizacion = token_Bow_MTDB[0]\n",
    "print(len(tokenizacion))\n",
    "print(len(tokenizacion[0]))\n",
    "print(len(tokenizacion[1]))\n",
    "print(len(tokenizacion[2]))\n",
    "print(tokenizacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hello': 6,\n",
       " 'hi': 3,\n",
       " 'class': 4,\n",
       " 'interesting': 5,\n",
       " 'like': 5,\n",
       " 'program': 1,\n",
       " 'good': 5,\n",
       " 'yet': 3,\n",
       " 'plan': 1,\n",
       " 'programming': 3,\n",
       " 'still': 2,\n",
       " 'race': 1}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bow = token_Bow_MTDB[1]\n",
    "print(len(Bow))\n",
    "Bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matriz Término Documento (binaria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0.],\n",
       "       [1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0.],\n",
       "       [0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1.]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_tdb = token_Bow_MTDB[2]\n",
    "print(mat_tdb.shape)\n",
    "mat_tdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dis_euc(tw):\n",
    "    i=0\n",
    "    while i < tw-1:\n",
    "        j = i+1\n",
    "        while j <= tw-1:\n",
    "            bin_cos = dot(mat_tdb[i],mat_tdb[j])/(norm(mat_tdb[i])*norm(mat_tdb[j]))\n",
    "            print(f\"El valor de la no se que del twit {i+1} contra el twit {j+1} es de : {bin_cos}\")\n",
    "            j=j+1\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El valor de la no se que del twit 1 contra el twit 2 es de : 0.0\n",
      "El valor de la no se que del twit 1 contra el twit 3 es de : 0.0\n",
      "El valor de la no se que del twit 1 contra el twit 4 es de : 0.3779644730092272\n",
      "El valor de la no se que del twit 1 contra el twit 5 es de : 0.0\n",
      "El valor de la no se que del twit 1 contra el twit 6 es de : 0.3779644730092272\n",
      "El valor de la no se que del twit 2 contra el twit 3 es de : 0.857142857142857\n",
      "El valor de la no se que del twit 2 contra el twit 4 es de : 0.5714285714285714\n",
      "El valor de la no se que del twit 2 contra el twit 5 es de : 0.857142857142857\n",
      "El valor de la no se que del twit 2 contra el twit 6 es de : 0.4285714285714285\n",
      "El valor de la no se que del twit 3 contra el twit 4 es de : 0.5714285714285714\n",
      "El valor de la no se que del twit 3 contra el twit 5 es de : 0.857142857142857\n",
      "El valor de la no se que del twit 3 contra el twit 6 es de : 0.4285714285714285\n",
      "El valor de la no se que del twit 4 contra el twit 5 es de : 0.7142857142857142\n",
      "El valor de la no se que del twit 4 contra el twit 6 es de : 0.857142857142857\n",
      "El valor de la no se que del twit 5 contra el twit 6 es de : 0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "dis_euc(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matriz Término Documento (frecuencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0.],\n",
       "       [1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0.],\n",
       "       [0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1.]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_tdf = token_Bow_MTDB[3]\n",
    "print(mat_tdf.shape)\n",
    "mat_tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_cos(tw):\n",
    "    i=0\n",
    "    while i < tw-1:\n",
    "        j = i+1\n",
    "        while j <= tw-1:\n",
    "            df_cos = dot(mat_tdf[i],mat_tdf[j])/(norm(mat_tdf[i])*norm(mat_tdf[j]))\n",
    "            print(f\"El valor de la no se que del twit {i+1} contra el twit {j+1} es de : {df_cos}\")\n",
    "            j=j+1\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El valor de la no se que del twit 1 contra el twit 2 es de : 0.0\n",
      "El valor de la no se que del twit 1 contra el twit 3 es de : 0.0\n",
      "El valor de la no se que del twit 1 contra el twit 4 es de : 0.3779644730092272\n",
      "El valor de la no se que del twit 1 contra el twit 5 es de : 0.0\n",
      "El valor de la no se que del twit 1 contra el twit 6 es de : 0.3779644730092272\n",
      "El valor de la no se que del twit 2 contra el twit 3 es de : 0.857142857142857\n",
      "El valor de la no se que del twit 2 contra el twit 4 es de : 0.5714285714285714\n",
      "El valor de la no se que del twit 2 contra el twit 5 es de : 0.857142857142857\n",
      "El valor de la no se que del twit 2 contra el twit 6 es de : 0.4285714285714285\n",
      "El valor de la no se que del twit 3 contra el twit 4 es de : 0.5714285714285714\n",
      "El valor de la no se que del twit 3 contra el twit 5 es de : 0.857142857142857\n",
      "El valor de la no se que del twit 3 contra el twit 6 es de : 0.4285714285714285\n",
      "El valor de la no se que del twit 4 contra el twit 5 es de : 0.7142857142857142\n",
      "El valor de la no se que del twit 4 contra el twit 6 es de : 0.857142857142857\n",
      "El valor de la no se que del twit 5 contra el twit 6 es de : 0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "sim_cos(6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
